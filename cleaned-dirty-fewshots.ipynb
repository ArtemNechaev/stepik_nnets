{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport shutil\n\nimport torchvision\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, Sampler, TensorDataset\nfrom torch.utils import data\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchvision import models\nfrom torchvision.utils import save_image\nfrom tqdm import tqdm, tqdm_notebook\nfrom sklearn.metrics import f1_score\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport copy\nimport random\nfrom itertools import combinations\n\n#from easyfsl.data_tools  import TaskSampler\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nprint(os.listdir(\"../input\"))\n\nimport zipfile\nwith zipfile.ZipFile('../input/platesv2/plates.zip', 'r') as zip_obj:\n    #Extract all the contents of zip file in current directory\n    zip_obj.extractall('/kaggle/working/')\n    \nprint('After zip extraction:')\nprint(os.listdir(\"/kaggle/working/\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-24T15:33:29.362907Z","iopub.execute_input":"2022-02-24T15:33:29.363163Z","iopub.status.idle":"2022-02-24T15:33:36.228931Z","shell.execute_reply.started":"2022-02-24T15:33:29.363135Z","shell.execute_reply":"2022-02-24T15:33:36.228004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"detector = models.detection.maskrcnn_resnet50_fpn(pretrained=True).to(DEVICE)\ndetector.eval()\n\ndetect_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n])\n\ndef circle_crop(img):\n    try:\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1.5, gray.shape[0] / 4,\n                                               param1=300, param2=20, minRadius=70)\n        x, y, r = circles[0, 0].astype('int32')\n        zoom = 1\n        #half_a = int(r * zoom/(2 ** 0.5))\n        half_a = int(r * zoom)\n        if (half_a > y) | (half_a + y > img.shape[0]) | (half_a > x) | (half_a + x > img.shape[1]):\n            raise Exception()\n            \n        mask1 = np.zeros_like(img)\n        mask1 = cv2.circle(mask1, (x,y), r, (1,1,1), -1)\n        \n        res = img * mask1\n        #Get the background\n        background = img - res\n        #Change all pixels in the background that are not black to white\n        background[np.where((background > [0, 0, 0]).all(axis = 2))] = mean * 255\n        #Add the background and the image\n        res = background + res\n        \n        res = res[y-half_a: y+half_a, x-half_a: x+half_a, :]\n        \n        return (res,0)\n    except:\n        tensor = detect_transforms(img).to(DEVICE)\n        pred = detector(tensor.unsqueeze(0))\n        boxes = pred[0]['boxes']\n        masks = pred[0]['masks']\n        sqare = ((boxes[:,2]-boxes[:,0])*(boxes[:,3]-boxes[:,1]))\n        if len(sqare) != 0:\n \n            boxes = boxes[sqare > 5000]\n            masks = masks[sqare > 5000]\n            x1, y1, x2, y2 =boxes[0].data.cpu().numpy().astype('int')\n\n            y2, y1 = img.shape[0] - y2, img.shape[0] - y1\n        \n            res = img * masks[0].permute(1, 2, 0).data.cpu().numpy().round()\n            background = img - res\n            #Change all pixels in the background that are not black to mean color\n            background[np.where((background > [0, 0, 0]).all(axis = 2))] = mean * 255\n            #Add the background and the image\n            res = background + res\n            #res = img\n            return (res[y2:y1, x1:x2, :],1)\n        else: \n            return img,2\n        \ndef crop_and_save(input_dir, output_dir, error_list):\n    for path in tqdm(os.listdir(input_dir)):\n        if path != '.DS_Store':\n            img = cv2.imread(os.path.join(input_dir, path))\n            img,check = circle_crop(img)\n            if check == 2:\n                erorrs.append(path)\n            cv2.imwrite(os.path.join(output_dir, path),img)  \n            \ndef image_fromtensor(input_tensor):\n    image = input_tensor.permute(1, 2, 0).data.cpu().numpy()\n    image = image*std + mean\n    return image\ndef show_input(input_tensor, title=''):\n    image = image_fromtensor(input_tensor)\n    plt.imshow(image)\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:33:36.231597Z","iopub.execute_input":"2022-02-24T15:33:36.2318Z","iopub.status.idle":"2022-02-24T15:33:47.726243Z","shell.execute_reply.started":"2022-02-24T15:33:36.231776Z","shell.execute_reply":"2022-02-24T15:33:47.72543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_sims(query_dataloader, support_dataloader, models = [], dist ='l2'):\n    sims = [] #N_Models x len(_q) x len(_s)\n    support_labels = []\n    with torch.set_grad_enabled(False):\n        for q_X, q_y in tqdm(query_dataloader):\n            q_X= q_X.to(DEVICE)\n            q_batch_size = q_y.shape[0]\n            sims_level1 = [] #N_Models x Batch_q x len(_s)\n            for s_X, s_y in support_dataloader:\n                s_X= s_X.to(DEVICE)\n                support_labels.append(s_y.data)\n                s_batch_size = s_y.shape[0]\n                sims_level2 = [] #N_Models x Batch_q x Batch_s\n                for m in models:\n                    q_z = m(q_X) #Batch_q x EmbSize\n                    s_z = m(s_X) #Batch_s x EmbSize\n\n                    q_z /= torch.linalg.norm(q_z, dim=1,  keepdim=True)\n                    s_z /= torch.linalg.norm(s_z, dim=1,  keepdim=True)\n\n                    if dist =='l2':\n                        dists = torch.cdist(q_z, s_z)\n                    else:\n                        dists = 1 - torch.mm(q_z, torch.t(s_z))\n                    sims_level2.append(dists)\n                sims_level2 = torch.cat(sims_level2).view(-1, q_batch_size, s_batch_size)\n                sims_level1.append(sims_level2)\n            sims_level1 = torch.cat(sims_level1, axis=-1)\n            sims.append(sims_level1)\n        sims = torch.cat(sims, dim =1)\n    return sims, torch.cat(support_labels)\n\ndef show_nearest(dataset_q, dataset_s, ind,  knn=5, start_index =0 , num_imgs = 10):\n    for i in range(num_imgs):\n        ii = start_index + i\n        q_img, q_y = dataset_q[ii]\n        fig, axs = plt.subplots(1,knn + 1,figsize=(20,12))\n        axs[0].imshow(image_fromtensor(q_img))\n        s_imgs =[ dataset_s[id][0] for id  in ind[ii,:knn]]\n        s_labels = [ dataset_s.targets[id] for id  in ind[ii,:knn]]\n        for ax_id in range(1, len(axs)):\n            axs[ax_id].imshow(image_fromtensor(s_imgs[ax_id-1]))\n            axs[ax_id].set_title(s_labels[ax_id-1])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:33:47.730604Z","iopub.execute_input":"2022-02-24T15:33:47.732558Z","iopub.status.idle":"2022-02-24T15:33:47.751987Z","shell.execute_reply.started":"2022-02-24T15:33:47.732511Z","shell.execute_reply":"2022-02-24T15:33:47.75116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"from typing import List, Tuple\n\ncombinations_of_classes = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)] # list(combinations(range(4), 2))\nweights_of_classes = [(0.3, 0.7), (0.5, 0.5), (0.3, 0.7), (0.7, 0.3), (0.5, 0.5), (0.3, 0.7)]\nmerged_classes = [ 1, 2, 3, 3, 3, 3]\n\nclass PlatesDataset(Dataset):\n    def __init__(self, data: List[Tuple[torch.Tensor, int]], transform=None, mix_up = False):\n        super().__init__()\n        if mix_up:\n            mixed = []\n            for couple in combinations(data, 2):\n                sample1, sample2  = sorted(couple, key= lambda item: item[1])\n                \n                comb_ids = (sample1[1],  sample2[1])\n                if comb_ids in combinations_of_classes:\n                    id = combinations_of_classes.index(comb_ids)\n                    weights = weights_of_classes[id]\n                    merged_class = merged_classes[id]\n                else:\n                    weights = (0.5, 0.5)\n                    merged_class = sample1[1]\n                    \n                one_mix = (sample1[0] * weights[0] + sample2[0] * weights[1], merged_class )\n                mixed.append(one_mix)\n            for d in data:\n                mixed.append(d)\n            data = mixed\n            \n        self.data = data            \n        self.transform = transform\n        self.targets = [y for X, y in data]\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        if self.transform:\n            x = self.transform(self.data[index][0])\n        else:\n            x = self.data[index][0]\n        y = self.data[index][1]\n        return x, y\n        \n\n\nclass TaskSampler(Sampler):\n    \"\"\"\n    Samples batches in the shape of few-shot classification tasks. At each iteration, it will sample\n    n_way classes, and then sample support and query images from these classes.\n    \"\"\"\n\n    def __init__(\n        self, dataset: Dataset, n_way: int, n_shot: int, n_query: int, n_tasks: int\n    ):\n        \"\"\"\n        Args:\n            dataset: dataset from which to sample classification tasks. Must have a field 'label': a\n                list of length len(dataset) containing containing the labels of all images.\n            n_way: number of classes in one task\n            n_shot: number of support images for each class in one task\n            n_query: number of query images for each class in one task\n            n_tasks: number of tasks to sample\n        \"\"\"\n        super().__init__(data_source=None)\n        self.n_way = n_way\n        self.n_shot = n_shot\n        self.n_query = n_query\n        self.n_tasks = n_tasks\n\n        self.items_per_label = {}\n        assert hasattr(\n            dataset, \"targets\"\n        ), \"TaskSampler needs a dataset with a field 'label' containing the labels of all images.\"\n        for item, label in enumerate(dataset.targets):\n            if label in self.items_per_label.keys():\n                self.items_per_label[label].append(item)\n            else:\n                self.items_per_label[label] = [item]\n\n    def __len__(self):\n        return self.n_tasks\n\n    def __iter__(self):\n        for _ in range(self.n_tasks):\n            yield torch.cat(\n                [\n                    # pylint: disable=not-callable\n                    \n                        torch.tensor(\n                        random.sample(\n                            self.items_per_label[label], (self.n_shot + self.n_query) )\n                        )\n                       \n                    \n                    \n                    # pylint: enable=not-callable\n                    for label in range(self.n_way) \n                ]\n            )\n\n    def episodic_collate_fn(\n        self, input_data: List[Tuple[torch.Tensor, int]]\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, List[int]]:\n        \"\"\"\n        Collate function to be used as argument for the collate_fn parameter of episodic\n            data loaders.\n        Args:\n            input_data: each element is a tuple containing:\n                - an image as a torch Tensor\n                - the label of this image\n        Returns:\n            tuple(Tensor, Tensor, Tensor, Tensor, list[int]): respectively:\n                - support images,\n                - their labels,\n                - query images,\n                - their labels,\n                - the dataset class ids of the class sampled in the episode\n        \"\"\"\n\n        true_class_ids = list({x[1] for x in input_data})\n\n        all_images = torch.cat([x[0].unsqueeze(0) for x in input_data])\n        all_images = all_images.reshape(\n            (self.n_way, self.n_shot + self.n_query, *all_images.shape[1:])\n        )\n        # pylint: disable=not-callable\n        all_labels = torch.tensor(\n            [true_class_ids.index(x[1]) for x in input_data]\n        ).reshape((self.n_way, self.n_shot + self.n_query))\n        # pylint: enable=not-callable\n\n        support_images = all_images[:, : self.n_shot].reshape(\n            (-1, *all_images.shape[2:])\n        )\n        query_images = all_images[:, self.n_shot :].reshape((-1, *all_images.shape[2:]))\n        support_labels = all_labels[:, : self.n_shot].flatten()\n        query_labels = all_labels[:, self.n_shot :].flatten()\n\n        return (support_images, support_labels,query_images), query_labels","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:33:47.756924Z","iopub.execute_input":"2022-02-24T15:33:47.759385Z","iopub.status.idle":"2022-02-24T15:33:47.796179Z","shell.execute_reply.started":"2022-02-24T15:33:47.759349Z","shell.execute_reply":"2022-02-24T15:33:47.795475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, freeze_bn, freeze_bn_affine, fc_backbone):\n        super().__init__()\n        \n        self.freeze_bn = freeze_bn\n        self.freeze_bn_affine = freeze_bn_affine\n        \n        self.ce_loss = nn.CrossEntropyLoss()\n        \n        #self.backbone = models.inception_v3(pretrained = True, aux_logits=False)\n        self.backbone = models.resnet50(pretrained = True)\n        #for param in self.backbone.parameters():\n            #param.requires_grad = False\n        #for param in self.backbone.layer4.parameters():\n            #param.requires_grad = True\n        if fc_backbone:\n            self.backbone.fc = fc_backbone\n            \n        \n        \n            \n    def train(self, mode=True):\n        \"\"\"\n        Override the default train() to freeze the BN parameters\n        \"\"\"\n        super().train(mode)\n        if self.freeze_bn:\n            for m in self.backbone.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n                    if self.freeze_bn_affine:\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n            #self.backbone.layer4.train()\n            \n    def forward(self):\n        pass\n    def loss_function(self, outputs, labels):\n        ce = self.ce_loss(outputs, labels)\n        return ce\n\n\nclass Proto_Net(Net):\n    \n    def __init__(self, freeze_bn = False, freeze_bn_affine = False, fc_backbone = False, dist='l2',  ):\n        super().__init__(freeze_bn = freeze_bn, freeze_bn_affine = freeze_bn_affine, fc_backbone = fc_backbone)\n        self.dist = dist\n        \n                        \n    def forward(self, x ):\n        support, support_labels, query = x\n        z_support = self.backbone(support)\n        z_query = self.backbone(query)\n        z_support = z_support/torch.linalg.norm(z_support, dim=1,  keepdim=True)\n        z_query = z_query/torch.linalg.norm(z_query, dim=1,  keepdim=True)\n        \n        # Infer the number of classes from the labels of the support set\n        n_way = len(torch.unique(support_labels))\n        # Prototype i is the mean of all support features vector with label i\n        self.z_proto = torch.cat(\n            [\n                z_support[torch.nonzero(support_labels == label)].mean(0)\n                for label in range(n_way)\n            ]\n        )\n        \n        # Compute the euclidean distance from queries to prototypes\n        if self.dist == 'l2':\n            dists = torch.cdist(z_query, self.z_proto)\n            scores = -dists\n            \n        # Compute the cos similarity from queries to prototypes   \n        elif self.dist == 'cos':\n            scores = torch.mm(z_query, torch.t(self.z_proto)) \n        \n        return scores\n        \n            \n            \nclass Clf_Net(Net):\n    def __init__(self, freeze_bn = False, freeze_bn_affine = False, fc_backbone = False,   ):\n        \n        super().__init__(freeze_bn = freeze_bn, freeze_bn_affine = freeze_bn_affine, fc_backbone = fc_backbone)\n        \n        self.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(2048,2))\n        \n    def forward(self, X):\n        X = self.backbone(X)\n        \n        return self.fc(X)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:33:47.801324Z","iopub.execute_input":"2022-02-24T15:33:47.803633Z","iopub.status.idle":"2022-02-24T15:33:47.82662Z","shell.execute_reply.started":"2022-02-24T15:33:47.8036Z","shell.execute_reply":"2022-02-24T15:33:47.825868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"code","source":"SEED = 0\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ndata_root = '/kaggle/working/plates/'\n\nbatch_size = 18\nlearning_rate = 0.0001\nnum_epoch = 30\ninput_size = 224\n\nnum_classes = 2 # 2 or 4\nN_SHOT = 5\nN_QUERY = 5\nN_TASKS = 5\n\nmix_up = True\nremove_strange_plates = True\nl2_reg = 0.001\n\nstd=np.array([0.229, 0.224, 0.225])\nmean=np.array([0.485, 0.456, 0.406])\n\nclass_names = ['cleaned', 'dirty']\n\nif num_classes == 4:\n    new_class_names = {'cleaned': 'patern_cleaned', 'dirty': 'patern_dirty'} \n    patern_ids ={'cleaned':  ['0006.jpg', '0007.jpg', '0008.jpg', '0009.jpg', '0011.jpg', '0013.jpg', '0016.jpg'], \n                 'dirty': ['0000.jpg', '0001.jpg', '0006.jpg', '0010.jpg', '0011.jpg', '0015.jpg', '0019.jpg']}  \n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:34:05.580894Z","iopub.execute_input":"2022-02-24T15:34:05.581148Z","iopub.status.idle":"2022-02-24T15:34:05.589201Z","shell.execute_reply.started":"2022-02-24T15:34:05.581123Z","shell.execute_reply":"2022-02-24T15:34:05.588495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#crop_train\n#cleaned\nerorrs = []\npath = os.path.join(data_root, 'train/cleaned')\ncrop_and_save(path, path, erorrs)\n\n#dirty\npath = os.path.join(data_root, 'train/dirty')\ncrop_and_save(path, path, erorrs)\n\nerorrs","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:34:10.819871Z","iopub.execute_input":"2022-02-24T15:34:10.820189Z","iopub.status.idle":"2022-02-24T15:34:18.49718Z","shell.execute_reply.started":"2022-02-24T15:34:10.820155Z","shell.execute_reply":"2022-02-24T15:34:18.496413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove strange plates and repeats\nif remove_strange_plates:\n    !rm '/kaggle/working/plates/train/cleaned/0000.jpg'\n    !rm '/kaggle/working/plates/train/cleaned/0004.jpg'\n    !rm '/kaggle/working/plates/train/cleaned/0006.jpg'\n    !rm '/kaggle/working/plates/train/dirty/0001.jpg'\n    !rm '/kaggle/working/plates/train/dirty/0003.jpg'\n    !rm '/kaggle/working/plates/train/dirty/0004.jpg'\n    !rm '/kaggle/working/plates/train/dirty/0010.jpg'\n    !rm '/kaggle/working/plates/train/dirty/0012.jpg'","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:34:25.249251Z","iopub.execute_input":"2022-02-24T15:34:25.249545Z","iopub.status.idle":"2022-02-24T15:34:30.770651Z","shell.execute_reply.started":"2022-02-24T15:34:25.249503Z","shell.execute_reply":"2022-02-24T15:34:30.769637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#devide data into 4 classes\nif num_classes == 4:\n    for cl_name in class_names:   \n        path = os.path.join(data_root, 'train', cl_name)\n        new_path = os.path.join(data_root, 'train', new_class_names[cl_name])\n        if not os.path.exists(new_path):\n            os.mkdir(new_path)\n        for file_name in os.listdir(path):\n            file_name = file_name\n            if file_name in patern_ids[cl_name]:\n                shutil.move(os.path.join(path, file_name) , new_path)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:34:31.979898Z","iopub.execute_input":"2022-02-24T15:34:31.980198Z","iopub.status.idle":"2022-02-24T15:34:31.988912Z","shell.execute_reply.started":"2022-02-24T15:34:31.980162Z","shell.execute_reply":"2022-02-24T15:34:31.988167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#crop test \n!rm -r test\ninpath = '/kaggle/working/plates/test'\noutpath = '/kaggle/working/test/unknown'\n\nos.mkdir('/kaggle/working/test/')\nos.mkdir(outpath)\n\nerorrs=[]\ncrop_and_save(inpath, outpath, erorrs)\n        \nlen(erorrs)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:34:33.949296Z","iopub.execute_input":"2022-02-24T15:34:33.950036Z","iopub.status.idle":"2022-02-24T15:35:22.92629Z","shell.execute_reply.started":"2022-02-24T15:34:33.949999Z","shell.execute_reply":"2022-02-24T15:35:22.925407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transforms\n\nshuffle_RGB = transforms.Lambda(lambda X: X[torch.randperm(3)])\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomRotation(360),\n    transforms.Resize(int(input_size * 1.3)),\n    transforms.CenterCrop(input_size),\n    #transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(0.05,0.05,0.1,0.1),\n    #transforms.ToTensor(),\n    transforms.RandomApply([shuffle_RGB], p=0.3),\n    transforms.Normalize(std=std, mean=mean),\n])\n\n\nbase_transforms = transforms.Compose([\n    transforms.Resize(int(input_size * 1.5)),\n    transforms.CenterCrop(input_size),\n    transforms.ToTensor(),\n    transforms.Normalize(std=std, mean=mean),\n])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T16:43:35.110395Z","iopub.execute_input":"2022-02-24T16:43:35.11067Z","iopub.status.idle":"2022-02-24T16:43:35.122298Z","shell.execute_reply.started":"2022-02-24T16:43:35.110642Z","shell.execute_reply":"2022-02-24T16:43:35.121549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_data\n\ntest_dir = 'test'\nif not os.path.exists(os.path.join(test_dir, 'unknown')):\n    shutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))\n\ntest_dataset = torchvision.datasets.ImageFolder('/kaggle/working/test', base_transforms)\npaths =[ p for p, i in test_dataset.imgs]\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:35:22.937693Z","iopub.execute_input":"2022-02-24T15:35:22.938529Z","iopub.status.idle":"2022-02-24T15:35:22.956122Z","shell.execute_reply.started":"2022-02-24T15:35:22.938496Z","shell.execute_reply":"2022-02-24T15:35:22.955425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_val data\n\nbase_dataset = torchvision.datasets.ImageFolder('plates/train', transform = base_transforms)\ntrain_dataset = PlatesDataset(base_dataset, train_transforms, mix_up = mix_up )\n    \n\nsampler = TaskSampler( train_dataset, n_way = num_classes, n_shot =  N_SHOT, n_query =  N_QUERY, n_tasks = N_TASKS)\nclf_dataloader = DataLoader(train_dataset, batch_size = 16, num_workers=2, shuffle = True)\nfsl_dataloader =  DataLoader(train_dataset, batch_sampler=sampler, num_workers=2,  collate_fn = sampler.episodic_collate_fn)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:35:22.958277Z","iopub.execute_input":"2022-02-24T15:35:22.958609Z","iopub.status.idle":"2022-02-24T15:35:23.42998Z","shell.execute_reply.started":"2022-02-24T15:35:22.958577Z","shell.execute_reply":"2022-02-24T15:35:23.429231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iter_data = iter(clf_dataloader)\nfor i in range(2):\n    q_img, q_y =next(iter_data)\n    grid_q = torchvision.utils.make_grid(q_img, scale_each=False, normalize=True)\n    fig, ax = plt.subplots(figsize=(20,12),)\n    \n    plt.imshow(grid_q.permute(1, 2, 0).cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:35:23.431315Z","iopub.execute_input":"2022-02-24T15:35:23.431601Z","iopub.status.idle":"2022-02-24T15:35:25.666821Z","shell.execute_reply.started":"2022-02-24T15:35:23.431567Z","shell.execute_reply":"2022-02-24T15:35:25.665916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nimport collections\n\n\ndef fit_epoch(model, train_loader, optimizer):\n    model.train()\n    running_loss = 0.0\n    running_f1 = 0\n  \n    for X, y in train_loader:\n        \n        if isinstance(X, collections.Sequence):\n            X = (x.to(DEVICE) for x in X)\n        else:\n            X = X.to(DEVICE)\n            \n        y = y.to(DEVICE)\n\n        optimizer.zero_grad()\n        outputs = model(X)\n        loss = model.loss_function(outputs, y)\n        \n        loss.backward()\n\n        optimizer.step()\n        running_loss += loss.item()\n        running_f1 += f1_score(y.cpu(), outputs.argmax(dim=1).cpu(), average ='micro')\n              \n    train_loss = running_loss / len(train_loader)\n    train_f1 = running_f1 / len(train_loader)\n    return train_loss, train_f1\n  \ndef eval_epoch(model, val_loader):\n    model.eval()\n    running_loss = 0.0\n    running_f1 = 0\n    for X, y in val_loader:\n        \n        if isinstance(X, collections.Sequence):\n            X = (x.to(DEVICE) for x in X)\n        else:\n            X = X.to(DEVICE)\n            \n        y = y.to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            outputs = model(X)\n            loss = model.loss_function(outputs, y)\n\n\n        running_loss += loss.item()\n        running_f1 += f1_score(y.cpu(), outputs.argmax(dim=1).cpu(), average ='micro')\n        \n\n    val_loss = running_loss / len(val_loader)\n    val_f1 = running_f1 / len(val_loader)\n    return val_loss, val_f1\n  \ndef train(train_loader, val_loader, model, epochs, l2_reg=0):\n    history = []\n    models = []\n    n_models = epochs // 5 if (epochs // 5) != 0 else epochs\n    best_model = None\n    best_loss = float('inf')\n    best_f1 = 0\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} val_loss {v_loss:0.4f} f1val {v_f1:0.4f}\"\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay  = l2_reg)\n    #sched = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n    \n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:        \n        for epoch in range(epochs):\n\n            train_loss, train_f1 = fit_epoch(model, train_loader, optimizer)\n            \n            #print(\"loss\", train_loss)            \n            val_loss, val_f1 = eval_epoch(model, val_loader)\n            sched.step(val_loss)\n            if (best_loss > val_loss):\n                best_loss = val_loss\n                best_model = copy.deepcopy(model)\n                print('new best model')\n            #if (best_f1 < train_f1):\n                #best_f1 = train_f1\n                #best_model = copy.deepcopy(model)\n            if epoch % n_models == 0:\n                models.append(model)\n            history.append([epoch, train_f1, val_f1])            \n            pbar_outer.update(1)\n\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, v_loss=val_loss, v_f1=val_f1))            \n    return history, best_model, models","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:35:25.668958Z","iopub.execute_input":"2022-02-24T15:35:25.669292Z","iopub.status.idle":"2022-02-24T15:35:25.691792Z","shell.execute_reply.started":"2022-02-24T15:35:25.669237Z","shell.execute_reply":"2022-02-24T15:35:25.691147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fc = nn.Dropout(0.2)\nmodel_0 = Proto_Net(freeze_bn = True, freeze_bn_affine = True, dist='l2', fc_backbone= fc ).to(DEVICE)\nh, b_model_0, proto_models = train(fsl_dataloader, fsl_dataloader, model_0 , num_epoch, l2_reg=l2_reg)\n#proto_models = [m.backbone for m in proto_models ]\n#proto_models.append(b_model_0.backbone)\n#proto_models.append(model_0.backbone)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T15:48:01.664244Z","iopub.execute_input":"2022-02-24T15:48:01.665021Z","iopub.status.idle":"2022-02-24T15:49:51.29075Z","shell.execute_reply.started":"2022-02-24T15:48:01.664973Z","shell.execute_reply":"2022-02-24T15:49:51.289793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_0.eval()\nb_model_0.eval()\nds = base_dataset\nsupport = DataLoader(ds, batch_size=16, shuffle = False)\nsims, support_labels = calc_sims(test_dataloader, support, models=[b_model_0.backbone, model_0.backbone], dist ='l2')\nind = sims.mean(0).argsort(1)\nprint(base_dataset.classes)\nshow_nearest(test_dataset, ds, ind,  knn=5, start_index =500 , num_imgs = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T17:13:46.497413Z","iopub.execute_input":"2022-02-24T17:13:46.497737Z","iopub.status.idle":"2022-02-24T17:17:10.043936Z","shell.execute_reply.started":"2022-02-24T17:13:46.497704Z","shell.execute_reply":"2022-02-24T17:17:10.042472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = np.repeat(np.array(ds.targets)[None,:],744, axis=0)\ntest_preds = np.take_along_axis(test_preds, ind.cpu().numpy(), axis=1)[:,:7]\n\ntest_preds = [np.argmax(np.bincount(t)) for t in test_preds]\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T17:17:35.765692Z","iopub.execute_input":"2022-02-24T17:17:35.766Z","iopub.status.idle":"2022-02-24T17:17:35.791056Z","shell.execute_reply.started":"2022-02-24T17:17:35.765968Z","shell.execute_reply":"2022-02-24T17:17:35.790193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if num_classes == 4:\n    class_names *= 2\ntest_preds = [ class_names[t] for t in test_preds]\n\nsubmission = pd.DataFrame({'label': test_preds, 'id': paths})\n#submission['label'] = submission['label'].map(lambda pred: 'dirty' if pred > 0.5 else 'cleaned')\nsubmission['id'] = submission['id'].str.replace('/kaggle/working/test/unknown/', '')\nsubmission['id'] = submission['id'].str.replace('.jpg', '')\nsubmission.set_index('id', inplace=True)\nsubmission[submission.label == 'dirty'].count()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T17:17:39.435448Z","iopub.execute_input":"2022-02-24T17:17:39.435795Z","iopub.status.idle":"2022-02-24T17:17:39.459226Z","shell.execute_reply.started":"2022-02-24T17:17:39.435763Z","shell.execute_reply":"2022-02-24T17:17:39.458204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T17:11:42.018623Z","iopub.execute_input":"2022-02-24T17:11:42.019371Z","iopub.status.idle":"2022-02-24T17:11:42.026809Z","shell.execute_reply.started":"2022-02-24T17:11:42.019333Z","shell.execute_reply":"2022-02-24T17:11:42.025959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r test plates","metadata":{"execution":{"iopub.status.busy":"2022-02-23T11:58:18.806004Z","iopub.execute_input":"2022-02-23T11:58:18.80634Z","iopub.status.idle":"2022-02-23T11:58:19.802482Z","shell.execute_reply.started":"2022-02-23T11:58:18.806302Z","shell.execute_reply":"2022-02-23T11:58:19.801362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}