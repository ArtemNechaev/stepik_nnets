{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random \nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n\nfrom transformers import AutoTokenizer\nfrom transformers import DataCollatorForSeq2Seq\nfrom transformers import AutoModelForSeq2SeqLM ","metadata":{"execution":{"iopub.status.busy":"2022-06-01T05:22:50.261544Z","iopub.execute_input":"2022-06-01T05:22:50.261869Z","iopub.status.idle":"2022-06-01T05:22:57.455951Z","shell.execute_reply.started":"2022-06-01T05:22:50.261801Z","shell.execute_reply":"2022-06-01T05:22:57.455233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 0\n\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ninput = '/kaggle/input/title-generation'\noutput ='/kaggle/working'\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-06-01T05:22:57.457495Z","iopub.execute_input":"2022-06-01T05:22:57.457731Z","iopub.status.idle":"2022-06-01T05:22:57.525416Z","shell.execute_reply.started":"2022-06-01T05:22:57.457697Z","shell.execute_reply":"2022-06-01T05:22:57.524707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(f'{input}/train.csv')\n\nTRAIN_SPLIT = int(len(df) * 0.97)\ndf.sample(frac=1).reset_index(drop=True)\n\ntrain_iter = df.iloc[:TRAIN_SPLIT]\nval_iter = df.iloc[TRAIN_SPLIT:]\n","metadata":{"execution":{"iopub.status.busy":"2022-06-01T05:22:57.52644Z","iopub.execute_input":"2022-06-01T05:22:57.526697Z","iopub.status.idle":"2022-06-01T05:22:59.771583Z","shell.execute_reply.started":"2022-06-01T05:22:57.526657Z","shell.execute_reply":"2022-06-01T05:22:59.770873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = train_iter.abstract.str.split().agg(len)\na.hist()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T05:22:59.773555Z","iopub.execute_input":"2022-06-01T05:22:59.773899Z","iopub.status.idle":"2022-06-01T05:23:03.169765Z","shell.execute_reply.started":"2022-06-01T05:22:59.773859Z","shell.execute_reply":"2022-06-01T05:23:03.16909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint ='t5-small'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T05:23:03.173629Z","iopub.execute_input":"2022-06-01T05:23:03.175759Z","iopub.status.idle":"2022-06-01T05:23:21.259649Z","shell.execute_reply.started":"2022-06-01T05:23:03.175719Z","shell.execute_reply":"2022-06-01T05:23:21.258921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n    prefix = \"summarize: \"\nelse:\n    prefix = \"\"\n\ndef preprocess_function(examples: pd.DataFrame):\n    inputs = [prefix + doc for doc in examples[\"abstract\"].values]\n    model_inputs = tokenizer(inputs, max_length=400, truncation=True,)\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(list(examples[\"title\"].values), max_length=128, truncation=True, )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nclass Sec2SecDataset(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = preprocess_function(data)\n    def __getitem__(self, idx):\n        return {k: v[idx] for k, v in  self.data.items()}\n    def __len__(self):\n        return len(list(self.data.values())[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-01T05:23:21.260898Z","iopub.execute_input":"2022-06-01T05:23:21.261229Z","iopub.status.idle":"2022-06-01T05:23:21.271691Z","shell.execute_reply.started":"2022-06-01T05:23:21.26119Z","shell.execute_reply":"2022-06-01T05:23:21.270055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Sec2SecDataset(train_iter)\nval_dataset = Sec2SecDataset(val_iter)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=20, collate_fn=data_collator)\nval_dataloader = DataLoader(val_dataset, batch_size=20, collate_fn=data_collator)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T05:23:21.27287Z","iopub.execute_input":"2022-06-01T05:23:21.273234Z","iopub.status.idle":"2022-06-01T05:24:37.457144Z","shell.execute_reply.started":"2022-06-01T05:23:21.273198Z","shell.execute_reply":"2022-06-01T05:24:37.456379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.optimization import AdamW\n\noptimizer = AdamW(model.parameters(), lr = 1e-4, weight_decay =0.000000001)\nmodel = model.to(device)\nmodel.load_state_dict(torch.load('/kaggle/input/titlegeneration-transformers/title-gen_model.pt', map_location=device))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T05:24:37.458393Z","iopub.execute_input":"2022-06-01T05:24:37.458642Z","iopub.status.idle":"2022-06-01T05:24:44.305962Z","shell.execute_reply.started":"2022-06-01T05:24:37.458608Z","shell.execute_reply":"2022-06-01T05:24:44.305278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\nimport matplotlib.pyplot as plt\n\n# Будем сохранять loss во время обучения\n# и рисовать график в режиме реального времени\ntrain_loss_set = []\ntrain_loss = 0\n\nval_loss_set = []\nx_val_set = []\n\n\n\n# Обучение\n# Переводим модель в training mode\nmodel.train()\n\n\nfor step, batch in enumerate(train_dataloader):\n    model.train()\n    # добавляем батч для вычисления на GPU\n    batch = {k: v.to(device) for k, v in batch.items()}\n    \n    # Распаковываем данные из dataloader\n    #b_input_ids, b_input_mask, b_labels, b_sent_ids = batch\n    \n    # если не сделать .zero_grad(), градиенты будут накапливаться\n    optimizer.zero_grad()\n    \n    # Forward pass\n    outputs = model(**batch)\n\n    train_loss_set.append(outputs.loss.item())  \n    \n    # Backward pass\n    outputs.loss.backward()\n    \n    # grad step\n    optimizer.step()\n\n    #  loss\n    train_loss += outputs.loss.item()\n    \n    #Evaluation\n    if step % 500 == 0:\n        val_loss = 0\n        for val_batch in val_dataloader:\n            val_batch = {k: v.to(device) for k, v in val_batch.items()}\n            model.eval()\n            with torch.no_grad():\n                outputs =  model(**val_batch)\n                val_loss += outputs.loss.item()\n        val_loss_set.append(val_loss/len(val_dataloader))\n        x_val_set.append(step)\n    \n    # Рисуем график\n    clear_output(True)\n    plt.plot(train_loss_set)\n    plt.plot(x_val_set,val_loss_set)\n    plt.title(\"Training loss\")\n    plt.xlabel(\"Batch\")\n    plt.ylabel(\"Loss\")\n    plt.show()\n    \nprint(\"Loss на обучающей выборке: {0:.5f}\".format(train_loss / len(train_dataloader)))\nprint(\"Loss на валидационной выборке: {0:.5f}\".format(val_loss / len(val_dataloader)))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T05:24:44.307302Z","iopub.execute_input":"2022-06-01T05:24:44.307559Z","iopub.status.idle":"2022-06-01T06:22:46.629731Z","shell.execute_reply.started":"2022-06-01T05:24:44.307523Z","shell.execute_reply":"2022-06-01T06:22:46.629059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'title-gen_model.pt')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T06:25:19.557456Z","iopub.execute_input":"2022-06-01T06:25:19.557946Z","iopub.status.idle":"2022-06-01T06:25:19.98187Z","shell.execute_reply.started":"2022-06-01T06:25:19.557905Z","shell.execute_reply":"2022-06-01T06:25:19.981139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LogitsProcessorList, LogitsProcessor\n\nclass MyLogProcessor(LogitsProcessor):\n    def __init__(self, encoder_inputs):\n        self.encoder_inputs_mask = torch.zeros((encoder_inputs.shape[0] * 5 , 32128))\n        for i in range (encoder_inputs.shape[0]):\n            self.encoder_inputs_mask[i:i+5, encoder_inputs[i]] = 1\n    def __call__(self, input_ids, scores):\n        if input_ids.shape[1] < 4:\n            #print(scores[self.encoder_inputs_mask == 0].mean(-1, keepdims=True))\n            scores[self.encoder_inputs_mask == 0] = -100#scores[self.encoder_inputs_mask == 0]  - scores[self.encoder_inputs_mask == 0].mean(-1, keepdims=True).abs()/2\n        return scores\n        \n\ndef predict_with_model(model, iterator, tokenizer,  device = None):\n    try:\n        model.eval()\n        device = model.device\n        condidate_corpus = []\n        ref_corpus = []\n        inputs = []\n\n        for batch in tqdm(iterator):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            #log_processor = LogitsProcessorList([MyLogProcessor(batch[\"input_ids\"])])\n            with torch.no_grad():\n                out = model.generate(input_ids=batch[\"input_ids\"],\n                                    attention_mask=batch[\"attention_mask\"],\n                                    do_sample=False, num_beams=5, max_length = 30,) #logits_processor = log_processor)\n            condidate_corpus +=tokenizer.batch_decode(out,skip_special_tokens=True)\n            ref_corpus += tokenizer.batch_decode(batch['decoder_input_ids'],skip_special_tokens=True)\n            inputs += tokenizer.batch_decode(batch[\"input_ids\"],skip_special_tokens=True)\n            \n        return condidate_corpus, ref_corpus, inputs\n    except KeyboardInterrupt:\n        return condidate_corpus, ref_corpus, inputs\n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-01T06:25:24.092452Z","iopub.execute_input":"2022-06-01T06:25:24.094446Z","iopub.status.idle":"2022-06-01T06:25:24.105494Z","shell.execute_reply.started":"2022-06-01T06:25:24.094382Z","shell.execute_reply":"2022-06-01T06:25:24.104741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"condidat_corpus, ref_corpus, inputs = predict_with_model(model, val_dataloader, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T06:25:29.172378Z","iopub.execute_input":"2022-06-01T06:25:29.172648Z","iopub.status.idle":"2022-06-01T06:28:22.58671Z","shell.execute_reply.started":"2022-06-01T06:25:29.172617Z","shell.execute_reply":"2022-06-01T06:28:22.585998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext.data.metrics import bleu_score\nref= [[r.split()] for r in ref_corpus]\npred = [c.split() for c in condidat_corpus]\n\nbleu_score(pred, ref, max_n=3, weights=[0.34, 0.33, 0.33])","metadata":{"execution":{"iopub.status.busy":"2022-06-01T06:29:47.350168Z","iopub.execute_input":"2022-06-01T06:29:47.350446Z","iopub.status.idle":"2022-06-01T06:29:48.805046Z","shell.execute_reply.started":"2022-06-01T06:29:47.350415Z","shell.execute_reply":"2022-06-01T06:29:48.804279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(30):\n    print(inputs[i], '\\n')\n    print(ref_corpus[i])\n    print(condidat_corpus[i])\n    print('')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T06:29:55.105452Z","iopub.execute_input":"2022-06-01T06:29:55.105718Z","iopub.status.idle":"2022-06-01T06:29:55.130852Z","shell.execute_reply.started":"2022-06-01T06:29:55.105687Z","shell.execute_reply":"2022-06-01T06:29:55.130157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data = pd.read_csv('/kaggle/input/title-generation/test.csv')\nsubmission_data['title'] = 'title title'","metadata":{"execution":{"iopub.status.busy":"2022-06-01T06:31:03.779306Z","iopub.execute_input":"2022-06-01T06:31:03.779875Z","iopub.status.idle":"2022-06-01T06:31:03.824041Z","shell.execute_reply.started":"2022-06-01T06:31:03.779836Z","shell.execute_reply":"2022-06-01T06:31:03.823361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data","metadata":{"execution":{"iopub.status.busy":"2022-06-01T06:31:06.07319Z","iopub.execute_input":"2022-06-01T06:31:06.073791Z","iopub.status.idle":"2022-06-01T06:31:06.089808Z","shell.execute_reply.started":"2022-06-01T06:31:06.073751Z","shell.execute_reply":"2022-06-01T06:31:06.089002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_dataset = Sec2SecDataset(submission_data)\ntest_iterator = DataLoader(test_dataset, batch_size=10, collate_fn = data_collator)\n\ntitles, _, _ = predict_with_model(model, test_iterator, tokenizer)\nsubmission_data['title'] = titles\nsubmission_data.to_csv('predicted_titles.csv', index=False)\n    \n#ubmission_df = pd.DataFrame({'abstract': abstracts, 'title': titles})\n#ubmission_df.to_csv('predicted_titles.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T06:31:16.476276Z","iopub.execute_input":"2022-06-01T06:31:16.47685Z","iopub.status.idle":"2022-06-01T06:32:10.293595Z","shell.execute_reply.started":"2022-06-01T06:31:16.476813Z","shell.execute_reply":"2022-06-01T06:32:10.292839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data","metadata":{"execution":{"iopub.status.busy":"2022-06-01T06:32:10.295273Z","iopub.execute_input":"2022-06-01T06:32:10.295757Z","iopub.status.idle":"2022-06-01T06:32:10.307924Z","shell.execute_reply.started":"2022-06-01T06:32:10.295718Z","shell.execute_reply":"2022-06-01T06:32:10.307002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = submission_data.join(train_iter.drop_duplicates(subset=['abstract']).set_index('abstract'), on = 'abstract', lsuffix='_pred', ).reset_index(drop=True)\nres.title = res.title.fillna(res.title_pred)\nres.pop('title_pred')\nres.to_csv('predicted_titles.csv', index=False)\nres","metadata":{"execution":{"iopub.status.busy":"2022-06-01T06:33:49.202751Z","iopub.execute_input":"2022-06-01T06:33:49.203021Z","iopub.status.idle":"2022-06-01T06:33:49.61181Z","shell.execute_reply.started":"2022-06-01T06:33:49.202991Z","shell.execute_reply":"2022-06-01T06:33:49.611108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nfrom nltk.util import ngrams\nimport numpy as np\nimport pandas as pd\nimport pickle\n\n\ndef generate_csv(input_file='predicted_titles.csv',\n                 output_file='submission.csv',\n                 voc_file='/kaggle/input/title-generation/vocs.pkl'):\n    '''\n    Generates file in format required for submitting result to Kaggle\n    \n    Parameters:\n        input_file (str) : path to csv file with your predicted titles.\n                           Should have two fields: abstract and title\n        output_file (str) : path to output submission file\n        voc_file (str) : path to voc.pkl file\n    '''\n    data = pd.read_csv(input_file)\n    with open(voc_file, 'rb') as voc_file:\n        vocs = pickle.load(voc_file)\n\n    with open(output_file, 'w') as res_file:\n        res_file.write('Id,Predict\\n')\n        \n    output_idx = 0\n    for row_idx, row in data.iterrows():\n        try:\n            trg = row['title']\n            trg = trg.translate(str.maketrans('', '', string.punctuation)).lower().split()\n            if len(trg) < 2:\n                trg *= 2\n            trg.extend(['_'.join(ngram) for ngram in list(ngrams(trg, 2)) + list(ngrams(trg, 3))])\n\n            VOCAB_stoi = vocs[row_idx]\n            trg_intersection = set(VOCAB_stoi.keys()).intersection(set(trg))\n            trg_vec = np.zeros(len(VOCAB_stoi))    \n\n            for word in trg_intersection:\n                trg_vec[VOCAB_stoi[word]] = 1\n\n            with open(output_file, 'a') as res_file:\n                for is_word in trg_vec:\n                    res_file.write('{0},{1}\\n'.format(output_idx, int(is_word)))\n                    output_idx += 1\n        except:\n            print(1)\n\ngenerate_csv()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T06:33:52.681746Z","iopub.execute_input":"2022-06-01T06:33:52.682299Z","iopub.status.idle":"2022-06-01T06:33:53.402591Z","shell.execute_reply.started":"2022-06-01T06:33:52.682259Z","shell.execute_reply":"2022-06-01T06:33:53.401831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}