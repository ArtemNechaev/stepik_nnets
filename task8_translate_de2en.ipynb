{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtemNechaev/stepik_nnets/blob/main/task8_translate_de2en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lytUO5mn9enL"
      },
      "source": [
        "# Translate German to English\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-31T15:40:21.558472Z",
          "iopub.status.busy": "2022-03-31T15:40:21.558096Z",
          "iopub.status.idle": "2022-03-31T15:40:24.189859Z",
          "shell.execute_reply": "2022-03-31T15:40:24.188975Z",
          "shell.execute_reply.started": "2022-03-31T15:40:21.558394Z"
        },
        "id": "HmgOOdMD3AmR",
        "outputId": "e39c6e16-6ec9-4457-b34b-a516c2f5197b",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'stepik_nnets' already exists and is not an empty directory.\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "ambiguous option: --v (--verbose, --version?)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ArtemNechaev/stepik_nnets & pip install -r requirements.txt\n",
        "#!pip install -U spacy \n",
        "#!python -m spacy download en_core_web_sm\n",
        "#!python -m spacy download de_core_news_sm \n",
        "#!wget 'http://vectors.nlpl.eu/repository/20/45.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-31T15:43:03.378307Z",
          "iopub.status.busy": "2022-03-31T15:43:03.378057Z",
          "iopub.status.idle": "2022-03-31T15:43:13.384384Z",
          "shell.execute_reply": "2022-03-31T15:43:13.383578Z",
          "shell.execute_reply.started": "2022-03-31T15:43:03.378279Z"
        },
        "id": "6dneov4I9enW",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ac4780f6-70dd-435c-c474-0855d114cbad"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1d16894c6951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#from torchtext.data import Field, BucketIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstepik_nnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msec2sec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslationDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequential_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorTransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyFastText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstepik_nnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msec2sec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstepik_nnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msec2sec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_gru\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOnlyGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stepik_nnets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from collections import OrderedDict, Counter\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, FastText, vocab\n",
        "from torchtext.datasets import Multi30k\n",
        "from typing import Iterable, List\n",
        "#from torchtext.data import Field, BucketIterator\n",
        "\n",
        "from stepik_nnets.sec2sec.data import TranslationDataset, sequential_transforms, TensorTransform, myFastText\n",
        "from stepik_nnets.sec2sec.models.base_model import Encoder, Decoder, Seq2Seq, Attention, MyAttention \n",
        "from stepik_nnets.sec2sec.models.only_gru import OnlyGRU \n",
        "\n",
        "from stepik_nnets.sec2sec.engine import train, evaluate, predict_with_model\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "import zipfile\n",
        "#import gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-31T15:43:23.647487Z",
          "iopub.status.busy": "2022-03-31T15:43:23.647201Z",
          "iopub.status.idle": "2022-03-31T15:43:23.659499Z",
          "shell.execute_reply": "2022-03-31T15:43:23.658735Z",
          "shell.execute_reply.started": "2022-03-31T15:43:23.647457Z"
        },
        "id": "LonnFFNd9enX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "pretrained_embed = True\n",
        "\n",
        "ENC_EMB_DIM = 300\n",
        "DEC_EMB_DIM = 100\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-31T15:43:29.449142Z",
          "iopub.status.busy": "2022-03-31T15:43:29.448386Z",
          "iopub.status.idle": "2022-03-31T15:43:29.456146Z",
          "shell.execute_reply": "2022-03-31T15:43:29.455202Z",
          "shell.execute_reply.started": "2022-03-31T15:43:29.449091Z"
        },
        "trusted": true,
        "id": "tzyfOrAPFh3d"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
        "\n",
        "\n",
        "def tokenize_text_simple_regex(txt, min_token_size=1):\n",
        "    txt = txt.lower()\n",
        "    all_tokens = TOKEN_RE.findall(txt)\n",
        "    return [token for token in all_tokens if len(token) >= min_token_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-31T15:44:41.637669Z",
          "iopub.status.busy": "2022-03-31T15:44:41.637395Z",
          "iopub.status.idle": "2022-03-31T15:44:45.677146Z",
          "shell.execute_reply": "2022-03-31T15:44:45.676384Z",
          "shell.execute_reply.started": "2022-03-31T15:44:41.63764Z"
        },
        "id": "Kc8Of-j_Qr6f",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e0347a-9307-44cf-8e61-ecb8f677f54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:181: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "ln_pair = (SRC_LANGUAGE, TGT_LANGUAGE)\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "\n",
        "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "\n",
        "token_transform[TGT_LANGUAGE] = tokenize_text_simple_regex#get_tokenizer('spacy', language='en_core_web_sm')\n",
        "token_transform[SRC_LANGUAGE] = tokenize_text_simple_regex#get_tokenizer('spacy', language='de_core_news_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "    \n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]].lower())\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "train_iter = ((token_transform[SRC_LANGUAGE](t[0]) , token_transform[TGT_LANGUAGE](t[1]))\n",
        "    for t in Multi30k(split='train', language_pair=ln_pair))\n",
        "\n",
        "train_iter_src, train_iter_trg = zip(*train_iter)\n",
        "\n",
        "\n",
        "if pretrained_embed:\n",
        "  ft_embed = myFastText(language='de')\n",
        "  vocab_transform[SRC_LANGUAGE] = vocab(ft_embed.stoi,\n",
        "                                                    min_freq=0,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "else:\n",
        "  vocab_transform[SRC_LANGUAGE] = build_vocab_from_iterator(train_iter_src,\n",
        "                                                      min_freq=1,\n",
        "                                                      specials=special_symbols,\n",
        "                                                      special_first=True)\n",
        "\n",
        "vocab_transform[TGT_LANGUAGE] = build_vocab_from_iterator(train_iter_trg,\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "\n",
        "\n",
        "#for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    #train_iter = Multi30k(split='train', language_pair=ln_pair)\n",
        "    # Create torchtext's Vocab object\n",
        "   # vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    #min_freq=1,\n",
        "                                                    #specials=special_symbols,\n",
        "                                                    #special_first=True)\n",
        "\n",
        "    # Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
        "    # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
        "\n",
        "vocab_transform[SRC_LANGUAGE].set_default_index(UNK_IDX)\n",
        "vocab_transform[TGT_LANGUAGE].set_default_index(UNK_IDX)\n",
        "\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in ln_pair:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               TensorTransform(BOS_IDX, EOS_IDX)) # Add BOS/EOS and create tensor\n",
        "\n",
        "if pretrained_embed:\n",
        "  src_embed = nn.Embedding(len(vocab_transform[SRC_LANGUAGE]), 300, padding_idx=PAD_IDX)\n",
        "  src_embed.load_state_dict({'weight': torch.cat([src_embed.weight[:4].data, ft_embed.vectors])})\n",
        "  src_embed.requires_grad_(False)\n",
        "else:\n",
        "  src_embed = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u,s,vt = torch.linalg.svd(ft_embed.vectors)"
      ],
      "metadata": {
        "id": "i0vbUuy0xYp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t5y8ySijxm1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z87CSNWgFh3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4de8947-7104-406e-c240-d8dcb8382aab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:181: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
          ]
        }
      ],
      "source": [
        "train_iter = Multi30k(split='train', language_pair=ln_pair)\n",
        "train_dataset = TranslationDataset( train_iter, text_transform, PAD_IDX, ln_pair = ln_pair)\n",
        "train_iterator = DataLoader(train_dataset, batch_size=30, shuffle =True, collate_fn = train_dataset.pad_collate_fn)\n",
        "\n",
        "valid_iter = Multi30k(split='valid', language_pair=ln_pair)\n",
        "val_dataset = TranslationDataset( valid_iter, text_transform, PAD_IDX, ln_pair = ln_pair)\n",
        "valid_iterator = DataLoader(val_dataset, batch_size=30, collate_fn = train_dataset.pad_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-31T15:44:55.667146Z",
          "iopub.status.busy": "2022-03-31T15:44:55.666406Z",
          "iopub.status.idle": "2022-03-31T15:44:55.680576Z",
          "shell.execute_reply": "2022-03-31T15:44:55.679565Z",
          "shell.execute_reply.started": "2022-03-31T15:44:55.667104Z"
        },
        "id": "k17tpfFWWtNz",
        "outputId": "4b2d037d-a3c2-4191-c056-941e2e20570a",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-30T11:25:15.526615Z",
          "iopub.status.busy": "2022-03-30T11:25:15.525878Z",
          "iopub.status.idle": "2022-03-30T11:25:15.797141Z",
          "shell.execute_reply": "2022-03-30T11:25:15.796419Z",
          "shell.execute_reply.started": "2022-03-30T11:25:15.526578Z"
        },
        "id": "snch6VGQ9enh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
        "OUTPUT_DIM = len(vocab_transform[TGT_LANGUAGE])\n",
        "\n",
        "ENC_HID_DIM = 100\n",
        "DEC_HID_DIM = 100\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT, embedding=src_embed)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn, embedding=None)\n",
        "\n",
        "model = Seq2Seq(enc, dec, PAD_IDX, BOS_IDX, EOS_IDX, device).to(device)\n",
        "#model = OnlyGRU(INPUT_DIM, OUTPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, PAD_IDX, EOS_IDX, device, embedding=src_embed).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not pretrained_embed:\n",
        "  def init_weights(m): \n",
        "    for name, param in m.named_parameters(): \n",
        "      if 'weight' in name: \n",
        "        nn.init.normal_(param.data, mean=0, std=0.01) \n",
        "      else: nn.init.constant_(param.data, 0)\n",
        "\n",
        "  model.apply(init_weights)"
      ],
      "metadata": {
        "id": "0iBo0rqCS5Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-30T11:07:28.082788Z",
          "iopub.status.busy": "2022-03-30T11:07:28.082384Z",
          "iopub.status.idle": "2022-03-30T11:07:28.089946Z",
          "shell.execute_reply": "2022-03-30T11:07:28.089162Z",
          "shell.execute_reply.started": "2022-03-30T11:07:28.082753Z"
        },
        "id": "0cp0OEmN9eni",
        "outputId": "76e623ab-5827-4644-de89-de4c7846c0a2",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель содержит 5,365,066 параметров\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'Модель содержит {count_parameters(model):,} параметров')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-30T11:25:21.263485Z",
          "iopub.status.busy": "2022-03-30T11:25:21.263222Z",
          "iopub.status.idle": "2022-03-30T11:25:21.268439Z",
          "shell.execute_reply": "2022-03-30T11:25:21.267753Z",
          "shell.execute_reply.started": "2022-03-30T11:25:21.263454Z"
        },
        "id": "JFfoCLut9enj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-30T11:07:29.171695Z",
          "iopub.status.busy": "2022-03-30T11:07:29.171416Z",
          "iopub.status.idle": "2022-03-30T11:07:29.180301Z",
          "shell.execute_reply": "2022-03-30T11:07:29.179427Z",
          "shell.execute_reply.started": "2022-03-30T11:07:29.171661Z"
        },
        "id": "qkJSQ4z99enk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-30T11:41:37.887253Z",
          "iopub.status.busy": "2022-03-30T11:41:37.886998Z",
          "iopub.status.idle": "2022-03-30T11:55:15.475951Z",
          "shell.execute_reply": "2022-03-30T11:55:15.474918Z",
          "shell.execute_reply.started": "2022-03-30T11:41:37.887225Z"
        },
        "id": "0qCCmsOB9enk",
        "outputId": "7578c5da-5a0e-493c-e071-e810396d2612",
        "scrolled": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 01 | Время: 0m 57s\n",
            "Перплексия (обучение):  16.208\n",
            "Перплексия (валидация):  53.192\n",
            "Эпоха: 02 | Время: 0m 58s\n",
            "Перплексия (обучение):  15.512\n",
            "Перплексия (валидация):  52.584\n",
            "Эпоха: 03 | Время: 0m 59s\n",
            "Перплексия (обучение):  14.988\n",
            "Перплексия (валидация):  52.396\n",
            "Эпоха: 04 | Время: 0m 57s\n",
            "Перплексия (обучение):  14.436\n",
            "Перплексия (валидация):  51.930\n",
            "Эпоха: 05 | Время: 0m 57s\n",
            "Перплексия (обучение):  14.128\n",
            "Перплексия (валидация):  50.942\n",
            "Эпоха: 06 | Время: 0m 57s\n",
            "Перплексия (обучение):  13.824\n",
            "Перплексия (валидация):  52.222\n",
            "Эпоха: 07 | Время: 0m 58s\n",
            "Перплексия (обучение):  13.439\n",
            "Перплексия (валидация):  51.744\n",
            "Эпоха: 08 | Время: 0m 57s\n",
            "Перплексия (обучение):  13.178\n",
            "Перплексия (валидация):  51.834\n",
            "Эпоха: 09 | Время: 0m 58s\n",
            "Перплексия (обучение):  12.868\n",
            "Перплексия (валидация):  52.391\n",
            "Эпоха: 10 | Время: 0m 57s\n",
            "Перплексия (обучение):  12.620\n",
            "Перплексия (валидация):  52.709\n",
            "Эпоха: 11 | Время: 0m 57s\n",
            "Перплексия (обучение):  12.336\n",
            "Перплексия (валидация):  52.301\n",
            "Эпоха: 12 | Время: 0m 57s\n",
            "Перплексия (обучение):  12.115\n",
            "Перплексия (валидация):  53.256\n",
            "Эпоха: 13 | Время: 0m 58s\n",
            "Перплексия (обучение):  11.901\n",
            "Перплексия (валидация):  54.096\n",
            "Эпоха: 14 | Время: 0m 58s\n",
            "Перплексия (обучение):  11.750\n",
            "Перплексия (валидация):  54.683\n",
            "Эпоха: 15 | Время: 0m 57s\n",
            "Перплексия (обучение):  11.601\n",
            "Перплексия (валидация):  53.669\n",
            "Эпоха: 16 | Время: 0m 58s\n",
            "Перплексия (обучение):  11.461\n",
            "Перплексия (валидация):  54.196\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-b2a4fc68db41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/stepik_nnets/sec2sec/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 50\n",
        "CLIP = float('inf')\n",
        "best_valid_loss = float('inf')\n",
        "model.forward_mode = 'next_word'\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, device)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion, device)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    sched.step(valid_loss)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'translate_model.pt')\n",
        "    \n",
        "    print(f'Эпоха: {epoch+1:02} | Время: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'Перплексия (обучение): {math.exp(train_loss):7.3f}')\n",
        "    print(f'Перплексия (валидация): {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-30T10:20:37.219406Z",
          "iopub.status.busy": "2022-03-30T10:20:37.219113Z",
          "iopub.status.idle": "2022-03-30T10:20:37.244997Z",
          "shell.execute_reply": "2022-03-30T10:20:37.244379Z",
          "shell.execute_reply.started": "2022-03-30T10:20:37.219372Z"
        },
        "id": "TofKSsiMHT-O",
        "outputId": "2a36abe4-50af-416b-83c6-81fda569ffe3",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-32e3028dd18b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'translate_model.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         loaded_storages[key] = torch.storage._TypedStorage(\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m             dtype=dtype)\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.54 GiB (GPU 0; 14.76 GiB total capacity; 10.51 GiB already allocated; 509.75 MiB free; 13.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('translate_model.pt', map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-30T11:35:36.651455Z",
          "iopub.status.busy": "2022-03-30T11:35:36.651202Z",
          "iopub.status.idle": "2022-03-30T11:35:37.811057Z",
          "shell.execute_reply": "2022-03-30T11:35:37.810189Z",
          "shell.execute_reply.started": "2022-03-30T11:35:36.651426Z"
        },
        "id": "nazwD_k39enk",
        "outputId": "104feb9d-5608-4742-9367-fa9a19d78aa9",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:181: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Перплексия (валидация):  54.251\n"
          ]
        }
      ],
      "source": [
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "test_dataset = TranslationDataset( test_iter, text_transform, PAD_IDX, ln_pair=ln_pair)\n",
        "test_iterator = DataLoader(test_dataset, batch_size=128, collate_fn = test_dataset.pad_collate_fn)\n",
        "\n",
        "#model.load_state_dict(torch.load('conala_model_attention_test.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Перплексия (валидация): {math.exp(test_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBqZI3iA9enl"
      },
      "source": [
        "## Translate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def predict( model, src, beam_size=5, max_len=20, device=None):\n",
        "  \"\"\"\n",
        "  src - src_len x batch_size\n",
        "\n",
        "  return tensor max_len x batch_size x beam_size\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  if device == None:\n",
        "      device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  src = src.to(model.device)\n",
        "  batch_size = src.shape[1]\n",
        "\n",
        "  # src_len x batch_size x hidden_size\n",
        "  encoder_outputs = model.encode(src) \n",
        "\n",
        "  # 1 x batch_size\n",
        "  input_trg = src[0].unsqueeze(0) \n",
        "  mask = model.create_pad_mask(src, input_trg)\n",
        "\n",
        "  # 1 x batch_size x vocab_size\n",
        "  first_decode, att, h_0 = model.decode(encoder_outputs, input_trg, mask, ) \n",
        "  first_decode = first_decode.log_softmax(-1)\n",
        "\n",
        "  # 1 x batch_size x beam_size\n",
        "  scores, indeces = first_decode.topk(beam_size) \n",
        "\n",
        "  # decoder hidden_state. num_layers * D, batch_size*beam_size, hidden\n",
        "  h_0 = h_0.unsqueeze(2).repeat(1,1,beam_size,1).reshape(h_0.shape[0], batch_size*beam_size, -1) \n",
        "\n",
        "  # src_len x batch_size*beam_size x hidden_size\n",
        "  encoder_outputs = encoder_outputs.unsqueeze(2).repeat(1,1,beam_size,1).reshape(src.shape[0], batch_size*beam_size, -1)\n",
        "\n",
        "  # src_len x batch_size*beam_size\n",
        "  src = src.unsqueeze(2).repeat(1,1,beam_size).reshape(src.shape[0], batch_size*beam_size)\n",
        "\n",
        "  for i in range(1, max_len):\n",
        "    input_trg = indeces[-1].view(1, batch_size * beam_size )\n",
        "    mask = model.create_pad_mask(src, input_trg)\n",
        "    dec_out , att, h_0 = model.decode(encoder_outputs, input_trg, mask, h_0=h_0)\n",
        "\n",
        "    # 1 x batch_size*beam_size x vocab_size\n",
        "    new_score = dec_out.log_softmax(-1) \n",
        "\n",
        "    # 1 x batch_size*beam_size x beam_size\n",
        "    new_score, new_idx = new_score.topk(beam_size)\n",
        "\n",
        "    new_score[:, torch.where(indeces[i-1].flatten() == model.eos_idx)[0]] = 0\n",
        "    scores = scores.reshape(1, batch_size*beam_size, 1) + new_score\n",
        "\n",
        "    lens_hyp = ((indeces != model.eos_idx) & (indeces != model.pad_idx)).sum(0, keepdim=True).reshape(1, batch_size*beam_size, 1)\n",
        "    scores = scores/(lens_hyp+1)**0.5\n",
        "\n",
        "    ######scores, new_idx = scores.topk(beam_size) #1 x  batch_size*beam_size x beam_size\n",
        "    flatten_scores = scores.reshape(1,batch_size, beam_size ** 2)\n",
        "\n",
        "    # 1 x batch_size x beam_size. order_scores [0 .. beam_size**2 - 1]\n",
        "    scores, order_scores = flatten_scores.topk(beam_size) \n",
        "\n",
        "    \n",
        "    # calc beam indexes that got top beam_size scores. beam_ids [0 .. beam_size - 1]\n",
        "    beam_ids = torch.div(order_scores, beam_size, rounding_mode='floor') \n",
        "\n",
        "    #select beams with top scores from indeces and concat new idx.\n",
        "    #start of sentences could repeat.\n",
        "\n",
        "    indeces = torch.cat([\n",
        "                         torch.gather(indeces, -1, beam_ids.repeat(indeces.shape[0], 1, 1)),\n",
        "                         torch.gather(new_idx.view(1, batch_size, -1),-1,order_scores)\n",
        "    ])\n",
        "\n",
        "    #select beams with top scores from decoder hidden_state\n",
        "    h_0 = h_0.reshape(h_0.shape[0], batch_size, beam_size, -1)\n",
        "    h_0 = torch.gather(h_0, 2, beam_ids.unsqueeze(3).repeat(h_0.shape[0], 1, 1, h_0.shape[3]))\n",
        "    h_0 = h_0.reshape(h_0.shape[0], batch_size*beam_size, -1)\n",
        "\n",
        "\n",
        "  return indeces, scores\n",
        "    \n",
        "def data_to_device (data, device):\n",
        "    if isinstance(data, Sequence):\n",
        "        data = (d.to(device) for d in data)\n",
        "    else:\n",
        "        data = data.to(device)\n",
        "    return data\n",
        "\n",
        "def predict_with_model(model, iterator, vocab: Vocab,  device = None):\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "    condidate_corpus = []\n",
        "    ref_corpus = []\n",
        "\n",
        "    for batch in iterator:\n",
        "      with torch.no_grad():\n",
        "          src, trg = data_to_device(batch, device)\n",
        "          #pred_trg, scores = predict(model,src, beam_size=1, max_len=trg.shape[0], device=device)\n",
        "          #pred_trg = pred_trg[:,:,0]\n",
        "\n",
        "          pred_trg = model(src, trg, 0)\n",
        "          pred_trg = pred_trg.argmax(-1)\n",
        "          \n",
        "          for i in range(src.shape[1]):\n",
        "              candidat = pred_trg[:,i][(pred_trg[:,i] != model.eos_idx) & (pred_trg[:,i] != model.pad_idx) ]\n",
        "              candidat = vocab.lookup_tokens(list(candidat.cpu().numpy()))\n",
        "\n",
        "              ref = trg[:,i][(trg[:,i] != model.eos_idx) & (trg[:,i] != model.pad_idx) ]\n",
        "              ref = vocab.lookup_tokens(list(ref.cpu().numpy()))\n",
        "\n",
        "              condidate_corpus.append(candidat)\n",
        "              ref_corpus.append([ref])\n",
        "    return condidate_corpus, ref_corpus\n"
      ],
      "metadata": {
        "id": "MvFTlmmsXQ2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.forward_mode = 'next_word'\n",
        "condidat_corpus, ref_corpus = predict_with_model(model, valid_iterator, vocab_transform['en'])"
      ],
      "metadata": {
        "id": "OLEooAeIcsr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score(condidat_corpus, ref_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV2cvKHXfSbh",
        "outputId": "d296a5ce-fcde-411f-baff-b85153e30e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10913214868767868"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-30T09:16:36.573477Z",
          "iopub.status.busy": "2022-03-30T09:16:36.572922Z",
          "iopub.status.idle": "2022-03-30T09:16:36.580327Z",
          "shell.execute_reply": "2022-03-30T09:16:36.579661Z",
          "shell.execute_reply.started": "2022-03-30T09:16:36.573436Z"
        },
        "id": "aD66UA029enl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def display_attention(candidate, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    \n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "   \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels([''] + ['<bos>'] + [t.lower() for t in token_transform['de'](candidate)] + ['<eos>'], \n",
        "                       rotation=45)\n",
        "    ax.set_yticklabels([''] + translation)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-30T11:35:51.860151Z",
          "iopub.status.busy": "2022-03-30T11:35:51.859886Z",
          "iopub.status.idle": "2022-03-30T11:35:51.886008Z",
          "shell.execute_reply": "2022-03-30T11:35:51.885175Z",
          "shell.execute_reply.started": "2022-03-30T11:35:51.860122Z"
        },
        "id": "Ng-5V79YmhrS",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa687fda-1ff3-4012-8a26-6d65131d19f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a i is a dog\n",
            "if a dog with a\n",
            "i am a dog with\n",
            "someone am a dog with\n",
            "i am a dog in\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "sent = 'ich habe ein hund'.lower()\n",
        "outputs, scores, a = model.predict(text_transform['de'](sent), beam_size = 5, max_len = len(sent.split(' ')) + 2)\n",
        "\n",
        "for i in range(outputs.shape[0]):\n",
        "    text = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(outputs[i].cpu().numpy()))\n",
        "    print(' '.join(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-30T11:36:10.515956Z",
          "iopub.status.busy": "2022-03-30T11:36:10.515701Z",
          "iopub.status.idle": "2022-03-30T11:36:12.396676Z",
          "shell.execute_reply": "2022-03-30T11:36:12.395962Z",
          "shell.execute_reply.started": "2022-03-30T11:36:10.515928Z"
        },
        "id": "uWrm-fGE9enl",
        "outputId": "e63f8f9c-b2df-446e-c340-cd4ddc1e92e8",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src = <bos> eine frau mit einer großen geldbörse geht an einem tor vorbei <eos>\n",
            "trg = <bos> a woman with a large purse is walking by a gate <eos>\n",
            "women with large purse walking past a gate <eos> a gate\n",
            "woman with large purse walking past a gate <eos> a gate\n",
            "lady with large purse walking past a gate <eos> a gate\n",
            "an woman with a large purse walks by a gate <eos>\n",
            "a woman with a large purse walks by a gate <eos>\n"
          ]
        }
      ],
      "source": [
        "example_idx = 8\n",
        "\n",
        "src, trg = train_dataset[example_idx]\n",
        "src_text, = ' '.join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src))), \n",
        "trg_text  = ' '.join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(trg)))\n",
        "\n",
        "print(f'src = {src_text}')\n",
        "print(f'trg = {trg_text}')\n",
        "\n",
        "outputs, scores, a = model.predict(src, beam_size = 5, max_len = 20)\n",
        "for i in range(outputs.shape[0]):\n",
        "    text = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(outputs[i].cpu().numpy()))\n",
        "    print(' '.join(text))\n",
        "    #display_attention(src_text, text, a[i])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-30T11:40:31.454757Z",
          "iopub.status.busy": "2022-03-30T11:40:31.454081Z",
          "iopub.status.idle": "2022-03-30T11:40:31.484493Z",
          "shell.execute_reply": "2022-03-30T11:40:31.483822Z",
          "shell.execute_reply.started": "2022-03-30T11:40:31.454718Z"
        },
        "id": "9kqBkP-U9enm",
        "outputId": "e3d44128-280a-4985-d7c2-9c6fd9030de0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src = <bos> einige männer sitzen in einem boot in der nähe eines strandes der mit gestapelten holzstämmen und stöcken bedeckt ist <eos>\n",
            "trg = <bos> some men are sitting on a boat near a beach covered in stacked logs and sticks <eos>\n",
            "many men sit in a boat near a beach with coconuts covered and hay <eos> <eos> hay\n",
            "a few men sit in a boat near a beach with wooden coconuts and covered in wood\n",
            "many men sit in a boat near a beach with coconuts covered and covered in coconuts <eos>\n",
            "few men sit in a boat near a beach with coconuts covered and covered in coconuts <eos>\n",
            "some men sit in a boat near a beach with coconuts covered and covered in coconuts <eos>\n"
          ]
        }
      ],
      "source": [
        "example_idx = 98\n",
        "src, trg = val_dataset[example_idx]\n",
        "src_text, = ' '.join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src))), \n",
        "trg_text  = ' '.join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(trg)))\n",
        "\n",
        "print(f'src = {src_text}')\n",
        "print(f'trg = {trg_text}')\n",
        "\n",
        "outputs, scores, a = model.predict(src, beam_size = 5, max_len = 40)\n",
        "for i in range(outputs.shape[0]):\n",
        "    text = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(outputs[i].cpu().numpy()))\n",
        "    print(' '.join(text))\n",
        "    #display_attention(src_text, text, a[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC, TRG = next(iter(test_iterator))"
      ],
      "metadata": {
        "id": "-91X7pVU6lCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 100\n",
        "\n",
        "src, trg = SRC[:,example_idx], TRG[:,example_idx]\n",
        "src_text, = ' '.join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src))), \n",
        "trg_text  = ' '.join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(trg)))\n",
        "\n",
        "print(f'src = {src_text}')\n",
        "print(f'trg = {trg_text}')\n",
        "\n",
        "outputs, scores, a = model.predict(src, beam_size = 5, max_len = 20)\n",
        "for i in range(outputs.shape[0]):\n",
        "    text = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(outputs[i].cpu().numpy()))\n",
        "    print(' '.join(text))\n",
        "    #display_attention(src_text, text, a[i])"
      ],
      "metadata": {
        "id": "IG-XdR03qYbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b42cbe-9f18-41e5-f305-dd0cc2dd50f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src = <bos> eine glückliche frau bereitet in einem coffee shop eine erfrischung zu <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "trg = <bos> a happy woman is preparing a <unk> at a coffee shop <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "a happy woman is preparing a drink in a coffee shop <eos> a coffee shop\n",
            "the beautiful woman is preparing a drink in a coffee shop <eos> a coffee shop\n",
            "the beautiful woman is preparing a drink at a coffee shop <eos> a coffee shop\n",
            "one woman is preparing a drink in a coffee store <eos> a coffee shop <eos>\n",
            "the beautiful woman is preparing a drink at a coffee shop <eos> <eos> <eos> <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx, scores = predict(model,SRC , beam_size=1, max_len=20, device=device)\n",
        "\n",
        "for i in range(idx.shape[2]):\n",
        "    text = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(idx[:,example_idx,i].cpu().numpy()))\n",
        "    print(' '.join(text))\n",
        "    #display_attention(src_text, text, a[i])\n",
        "\n"
      ],
      "metadata": {
        "id": "OnS81U6lCPA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d993c5-02a4-4f75-eb53-5142282da661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a happy woman is preparing a drink in a coffee shop <eos> a coffee shop <eos> <eos> <eos> <eos> <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#src.unsqueeze(1)\n",
        "idx, scores = predict(model,src.unsqueeze(1) , beam_size=5, max_len=20, device=device)\n",
        "\n",
        "for i in range(idx.shape[2]):\n",
        "    text = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(idx[:,0,i].cpu().numpy()))\n",
        "    print(' '.join(text))\n",
        "    #display_attention(src_text, text, a[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpHnc6tY-3M9",
        "outputId": "add0158c-df8a-44d3-a0e1-b8da8c5cbd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a happy woman preparing a drink at a coffee shop <eos> a coffee shop <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "a happy woman preparing a drink at a coffee shop <eos> a coffee store <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "a happy woman is preparing a drink at a coffee shop <eos> a coffee shop <eos> <eos> <eos> <eos> <eos>\n",
            "a happy woman is preparing a drink in a coffee shop <eos> a coffee shop <eos> <eos> <eos> <eos> <eos>\n",
            "a happy woman is preparing a drink in a coffee store <eos> a coffee shop <eos> <eos> <eos> <eos> <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LYNVOFDbcKg3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "name": "task8 translate de2en.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}