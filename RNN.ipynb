{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtemNechaev/stepik_nnets/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cz9PwvqshMb"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "import sys; sys.path.append('./stepik-dl-nlp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HaHYYcfkHpGf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import math\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
        "\n",
        "init_random_seed(765)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YWdCZJikvBi6"
      },
      "outputs": [],
      "source": [
        "input_file =  open('./stepik-dl-nlp/datasets/author_quotes.txt') \n",
        "quotes = input_file.read()[:-1].split('\\n')\n",
        "##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aaVbbV0hJP0x",
        "outputId": "4dfbbebb-e07d-4e6f-f7ef-ef2aa9b5b332"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'did, ,you, ,ever, ,stop, ,to, ,thin,k, ,and, ,forg,et, ,to, ,star,t, ,agai,n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = re.compile(r'[\\w\\d]{1,4}|\\s')\n",
        "tokenize_quotes = [tokenizer.findall(q.lower()) for q in quotes]\n",
        "\",\".join(tokenize_quotes[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWuU0obBI-pc",
        "outputId": "af64602a-dc17-44f4-ad2a-5a1abdcb76f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('zuko', 12191),\n",
              " ('zulu', 12192),\n",
              " ('zure', 12193),\n",
              " ('zy', 12194),\n",
              " ('zzi', 12195)]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = list(np.unique(np.concatenate(tokenize_quotes)))\n",
        "vocab = ['<PAD>', '<UNK>', '<BEGIN>', '<END>'] + vocab\n",
        "vocab = {v: i for i, v in enumerate(vocab)}\n",
        "list(vocab.items())[-5:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "K2hnlqX6vzWI"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "class SeqDataset(Dataset):\n",
        "  def __init__(self, data, vocab: Dict):\n",
        "    super().__init__()\n",
        "    max_length = max([ len(d) for d in data ])\n",
        "    self.data = torch.zeros((len(data), max_length + 2), dtype=torch.long)\n",
        "    self.data[:,0] = 2\n",
        "    for n_sent, sentence in enumerate(data):\n",
        "      for n_token, token in enumerate(sentence):\n",
        "        self.data[n_sent, n_token + 1] = vocab.get(token, 1)\n",
        "      self.data[n_sent, n_token + 2] = 3\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.data.shape[0] - 1\n",
        "  def __getitem__(self, id):\n",
        "    return self.data[id, :-1], self.data[id, 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_FgaAyS5r0G",
        "outputId": "5459e306-97bf-4511-85f6-c46074962298"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([    2,  4998,  6496,     4, 10292,  5975,     4, 11546, 10225,     4,\n",
              "          4057,     4, 10640,     4,  7181,  4642,  4855,     4,  7022,     4,\n",
              "          8218,  7529,     4,  8132,     4, 10640,     4,  9872,     4,   780,\n",
              "             4,  4998,  6496,     4, 10651,  2921,     4,   682,     4, 10640,\n",
              "             4, 10735,     4, 10637,     4,  4998,     4, 11949,  3031,     4,\n",
              "          5261,     4, 11849,     4,  4998,     4, 11774,     4,   332,     4,\n",
              "         10573,   538,     4,  4998,     4,  4624,     4,   332,     4,  1960,\n",
              "          2355,     4,  4057,     4,  7022,     4,  1960,  2586,  2355,     4,\n",
              "         10416, 10641,  9437,     3,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]),\n",
              " tensor([ 4998,  6496,     4, 10292,  5975,     4, 11546, 10225,     4,  4057,\n",
              "             4, 10640,     4,  7181,  4642,  4855,     4,  7022,     4,  8218,\n",
              "          7529,     4,  8132,     4, 10640,     4,  9872,     4,   780,     4,\n",
              "          4998,  6496,     4, 10651,  2921,     4,   682,     4, 10640,     4,\n",
              "         10735,     4, 10637,     4,  4998,     4, 11949,  3031,     4,  5261,\n",
              "             4, 11849,     4,  4998,     4, 11774,     4,   332,     4, 10573,\n",
              "           538,     4,  4998,     4,  4624,     4,   332,     4,  1960,  2355,\n",
              "             4,  4057,     4,  7022,     4,  1960,  2586,  2355,     4, 10416,\n",
              "         10641,  9437,     3,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = SeqDataset(tokenize_quotes, vocab)\n",
        "\n",
        "train_size = int(len(dataset)*0.9)\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) -  train_size])\n",
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pTggfid1v5wl"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size = 64, h_size = 64 ):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
        "    self.RNN = nn.LSTM(emb_size, h_size, batch_first=True)\n",
        "    self.fc = nn.Linear(h_size, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    x - tensor BatchSize x MaxSeqLen\n",
        "\n",
        "    \"\"\"\n",
        "    h, _ = self.RNN(self.embed(x))\n",
        "    logits = self.fc(h)\n",
        "\n",
        "    return logits.permute(0,2,1)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "j9eDmxyeABRU"
      },
      "outputs": [],
      "source": [
        "rnn = Model(len(vocab))\n",
        "loss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_KdqfgzDZ8s",
        "outputId": "7de99b99-4c03-4efc-af81-8fe2661cfc6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпоха 0\n",
            "Эпоха: 255 итераций, 93.13 сек\n",
            "Среднее значение функции потерь на обучении 2.0368499540815166\n",
            "Среднее значение функции потерь на валидации 1.177451228273326\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 1\n",
            "Эпоха: 255 итераций, 93.08 сек\n",
            "Среднее значение функции потерь на обучении 1.1083298814062978\n",
            "Среднее значение функции потерь на валидации 1.0339915752410889\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 2\n",
            "Эпоха: 255 итераций, 92.79 сек\n",
            "Среднее значение функции потерь на обучении 0.9998902909895953\n",
            "Среднее значение функции потерь на валидации 0.9567129057029198\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 3\n",
            "Эпоха: 255 итераций, 92.87 сек\n",
            "Среднее значение функции потерь на обучении 0.9353294603964861\n",
            "Среднее значение функции потерь на валидации 0.9068694587411552\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 4\n",
            "Эпоха: 255 итераций, 92.95 сек\n",
            "Среднее значение функции потерь на обучении 0.8952510494811862\n",
            "Среднее значение функции потерь на валидации 0.8780339154703863\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 5\n",
            "Эпоха: 255 итераций, 92.82 сек\n",
            "Среднее значение функции потерь на обучении 0.868853219350179\n",
            "Среднее значение функции потерь на валидации 0.8579367008702509\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 6\n",
            "Досрочно остановлено пользователем\n"
          ]
        }
      ],
      "source": [
        "(best_val_loss,\n",
        " rnn_best_model) = train_eval_loop(rnn,       train_dataset,\n",
        "                                            val_dataset,\n",
        "                                            loss,\n",
        "                                            lr=2e-3,\n",
        "                                            epoch_n=10,\n",
        "                                            batch_size=128,\n",
        "                                            device='cuda',\n",
        "                                            early_stopping_patience=30,\n",
        "                                            max_batches_per_epoch_train=500,\n",
        "                                            max_batches_per_epoch_val=100,\n",
        "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim,\n",
        "                                                                                                                         verbose=True ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "u87h9PG7D9G6"
      },
      "outputs": [],
      "source": [
        "def generarate_text(generator, temperature=1, max_length = 30):\n",
        "  seq = vocab.get('<BEGIN>', 2)\n",
        "  seq = torch.tensor([[seq]], dtype=torch.long).cuda()\n",
        "  k_list = list(vocab.keys())\n",
        "  for i in range(max_length):\n",
        "    probs = (generator(seq).permute(0,2,1)[0,-1]/temperature).softmax(-1).data.cpu().numpy()\n",
        "    new_token = np.random.choice(len(vocab), p = probs)\n",
        "    if new_token == 3:\n",
        "      return ''.join([k_list[ix] for ix in seq.data.cpu().numpy()[0] if ix != 2] )\n",
        "    new_token = torch.tensor([[new_token]], dtype=torch.long).cuda()\n",
        "    seq = torch.cat([seq, new_token], dim=1)\n",
        "    \n",
        "\n",
        "  return ''.join([k_list[ix] for ix in seq.data.cpu().numpy()[0] if ix != 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojlBKegVD80P",
        "outputId": "5ed79067-6110-4e1d-a5ad-3aeb06337ae8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i see not home to be an great extrentertaity of you go to do the rule\n",
            "when i know i think there to do you just i have tours that i dont enjoy a less still i do in the past of the very continue and i administ and there a lot of your greatest of a really little named that your own i dont do like it when they  would last it and that esses of it i really my law it is to be everyone is the country if you was a about the playksgin by the most \n",
            "i want and much in the world on in the experience and i did playing that i understanding to be it must my feel is a galle of the art of away to the potential i have am laugh in those it it back and become has that one still at me out and to say my films but i nation\n",
            "the dull of the number of the mean of the much ideas to be to begins that everyone is a york but i saw us past and i didnt have my old father that painting in my beautiful of his characters and the little day can do that to bring it it are no job of an prefer in the world and a as well what i was not that you am a lot of will a lot and i can hang for the the smart play i\n",
            "i would know they just can be travel my much do to come to the  your faces at the do to the time to do your thta of me in the radio doesn if you have a politics and the mind\n",
            "i have have like the song of my self i dont never think you creates you are every the get and that the same way leave and you must sing is not i done what i are a lot of the life what i are a lot of much or all the past of life\n",
            "i should be understand and you have the time because they do he down and the good system on the lot of the physical people and my american positions and people is always looking in an time and i can always more sense of them an old many other of art with an role in a take a cannot completely i was just quite of your love of the future and you do not a lot of art and the free of seek is still\n",
            "i were a lot of the value of being the be damn of the father it a come in the world\n",
            "i have in the political responsible and me what the mouth is the early who need doesnt of the social of a job that i have a universe is a small interested and right was kind of the way in the three children of the not say the computer and you was a lack people are the master\n",
            "i really want\n"
          ]
        }
      ],
      "source": [
        "for _ in range(10):\n",
        "    print(generarate_text(rnn_best_model, temperature=0.7, max_length = max([ len(_) for _ in tokenize_quotes ])), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41tCdEh2D8xT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNngS9y5VgARS9DWL6bk8KN",
      "include_colab_link": true,
      "name": "RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
